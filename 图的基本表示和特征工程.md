# 1. 图的基本表示

- 图的本体设计
- 图的种类（有向、无向、异质、二分、连接带权重)
- 节点连接数
- 图的基本表示-邻接矩阵
- 图的基本表示-连接列表和邻接列表
- 图的连通性

![1676335747223](image\1676335747223.png)

> 图 G（网络）由节点 N（顶点）和边 E（连接）构成；

图是一种通用的语言。

![1676336283522](image\1676336283522.png)

> 电影角色关系、人物社交网络、蛋白质作用都可以抽象成同一个图。
>
> 这个图的节点数是4，连接数是4

## 1.1 本体设计

​	图并不是凭空产生的、也不是现成的，是需要我们人去设计、创造。如何判断以哪些信息为节点、哪些信息为连接就要涉及到图的本体Ontology。

​	以医疗知识图谱为例：

![1676338594681](image\1676338594681.png)

> 红色节点：疾病；蓝色节点：食物；绿色节点：症状；粉色节点：科室...
>
> 疾病节点和食物节点之间有三种连接：能吃、不能吃、推荐吃。
>
> 在设计知识图谱之初，我们就应该把节点的类型、以及节点之间可能存在的关系都设计好。

<img src="image\1676339263483.png" alt="1676339263483" style="zoom: 67%;" />

> 上面就是本体图，这是我们在导入数据之前就要设计好的。具体的图只不过是本体图的具象化，把疾病节点填写成了肺曲霉病、肺气肿等；

![1676339751695](image\1676339751695.png)

> 如何设计本体Ontology却决于之后想解决什么样的问题。
>
> 比如说一个医疗QA知识图谱，“肺气肿可以吃什么”——疾病和食物都要作为节点。
>
> 有些时候，本体图是唯一的，没有歧义的。
>
> 比如说：社交网络，人就是一个节点。没有别的本体图的设计范式了。
>
> 如果说要做一个红楼梦的知识图谱，就要把四大家族考虑进去，还要把地点、事件考虑进去。在这个图里面，节点类型就不只有人，还有地点、事件了。

## 1.2 图的种类

### 1.2.1 有向图、无向图

![1676347530911](image\1676347530911.png)

- 无向图：
  - 连接是无向的（对称，双向）
  - ep：论文合作关系；Facebook上的好友关系
- 无向图：
  - 连接有方向
  - ep：A = 打电话 => B；Twitter上的关注关系

### 1.2.2 异质图

![1676348355193](image\1676348355193.png)

- 节点存在不同的类型；
- 连接存在不同的类型；
- ep：医疗知识图谱；红楼梦知识图谱。

![1676348849875](image\1676348849875.png)

- 医疗知识图谱、论文应用都是异质图

### 1.2.3 二分图

二分图：节点种类为2的异质图。

![1676348783633](image\1676348783633.png)

Node: Protein、Drug.

![1676348928394](image\1676348928394.png)

- 二部图是一个节点可以被分成两个不相交的集合U和v，使得每个链接都将U中的一个节点连接到V中的一个节点，也就是说，U和v是独立集。
- Example：
  - 作者-论文
  - 导演-电影
  - 用户-电影
  - 菜谱-食材
- 二分图可以展开

<img src="image\1676349258621.png" alt="1676349258621" style="zoom:67%;" />

> 比如说U是作者，V是论文。
>
> 那么我们把所有跟同一个论文有关的作者建立一条连接，如1和2都和A论文有关系，就把1和2建立连接；2和3都和A有关系，那么2和3就建立一条连接，3和1都和A有关，那么3和1也建立一条连接；2和5都与B有关，那么2和5就建立一条连接。
>
> 同理我们也可以把V展开，A和B都与2有关系，那么A和B之间有连接。
>
> 这样我们就把U展成了一个图，把V展成了一个图。而且U和V都不是二分图，都是只有一类节点组成的。那么我们就可以分别对这两张图进行数据挖掘。

### 1.2.4 节点连接数

节点连接数，也称为度。

![1676360161241](image\1676360161241.png)

**对于无向图：**

- ==节点连接数==：一个节点存在多少连接。
  - 如：A节点，$k_A=4$
- 平均连接数：$\displaystyle \bar{k}=<k>=\frac{1}{N}\sum_{i=1}^NK_i=\frac{2E}{N}$

**对于有向图：**

有向图中有：in-degree（入度）和out-degree（出度）；in-degree + out-degree = total degree（总度）

- In-degree：指向改节点的边的个数。
  - 如：$k_C^{in}=2$  ;  $k_C^{out}=1$  ;  $k_C = k_C^{in} + k_C^{out} = 3$
- Out-degree：由该节点发出的边的个数。
- Source：节点的入度为0，$k^{in}=0$；（发送点，即源头）
- Sink：节点的出度为0，$k^{out}=0$；（接收端，即沟渠）
- 平均连接数：$\bar{k}=\frac{E}{N}$；
- 平均出度=平均入度：$\bar{k^{in}}=\bar{k^{out}}$

<img src="image\1676361833698.png" alt="1676361833698" style="zoom:50%;" />

> 我们可以使用Node Degree来表示一个节点的重要度。如果一个节点的连接数很多，证明这个节点在这个图中起到的作用也就越大。比如说：在一个社交网络中，认识的人越多，重要性也就越大。

### 1.2.5 图的矩阵表示

 **为什么要把图表示成矩阵的形式？** 

邻接矩阵保留了图的所有信息，算法和计算机不知道什么是图，他接受的数据只能是结构化的（如：矩阵）。我们常用的计算框架：NumPy、PyTorch、CUDA都是为了解决矩阵运算加速的问题。我们首先要把图转换为算法和计算机能够输入的数据形式——矩阵，才能够进行后续的处理。现阶段的深度学习，其实就是加速矩阵的乘法、分解等。所以说我们把图表示成矩阵的形式，就是把图翻译成了计算机的语言。

#### 1.2.5.1 邻接矩阵

![1676361987015](image\1676361987015.png)

邻接矩阵：如果节点 $i$ 和节点 $j$ 之间有连接，则$A_{ij}=1$，否则为0.矩阵的Size为：N×N。

**无向图：**

- 邻接矩阵：对称矩阵 $A_{ij}=A_{ji}$；没有环时：$A_{ii}=0$

<img src="image\1676366109398.png" alt="1676366109398" style="zoom:67%;" />

- 连接总数：$L=\frac{1}{2}\displaystyle\sum_{i=1}^Nk_i=\frac{1}{2}\sum_{ij}^NA_{ij}$
- 第i个节点的度数：对第i行或第i列求和：$k_i=\displaystyle\sum_{j=1}^NA_{ij}$；$k_j=\displaystyle\sum_{i=1}^NA_{ij}$

**有向图：**

- 邻接矩阵：非对称矩阵$A_{ij}\not=A_{ji}$；没有环时：$A_{ii}=0$

<img src="image\1676366090282.png" alt="1676366090282" style="zoom:67%;" />

- 连接总数：$L=\displaystyle\sum_{i=1}^Nk_i^{in}=\sum_{j=1}^Nk_i^{out}=\sum_{ij}^NA_{ij}$
- 第i个节点的出度：第i行数据求和，$k_{i}^{\text {out }}=\displaystyle\sum_{j=1}^{N} A_{i j}$
- 第i个节点的入度：第j列数据求和；$k_{j}^{\text {in}}=\displaystyle\sum_{i=1}^{N} A_{i j}$

**缺陷：**

<img src="image\1676368117909.png" alt="1676368117909" style="zoom:50%;" />

大自然中绝大部分的图都是稀疏的。

如果我们要构建世界上所有人的社交网络，假设每个人认识不超过1000人。

那么我们的邻接矩阵就是70亿×70亿大小的矩阵，那么这个矩阵是十分稀疏的。

<img src="image\1676368324103.png" alt="1676368324103" style="zoom:67%;" />

对于其他例子也是如此，以因特网为例，节点平均度数$<k>=\frac{2L}{N}=6.33$。浩瀚的因特网，每个连接数也只不过是6。所以这是一个非常稀疏的网络。

如果计算矩阵密度，将会得到一个特别小的数字。

#### 1.2.5.2 连接列表和邻接列表

<img src="image\1676369221337.png" alt="1676369221337" style="zoom:67%;" /> 

- 连接列表列出的是节点的指向关系：$（V_1,V_2）$也就是由$V_1$指向$V_2$.

<img src="image\1676369331161.png" alt="1676369331161" style="zoom:67%;" /> 

- 邻接列表列出的是所有节点的指向关系。
- 使用邻接列表时，如果网络（图）很大、很稀疏那么就会很便捷。
- 能够使我们更快速的定位给定节点的所有邻居节点。

<img src="image\1676369557506.png" alt="1676369557506" style="zoom:50%;" />

---

不论是邻接矩阵、连接列表还是邻接列表，他们反应的都是图的连接信息，构建出来图应该也是同一个。

### 1.2.6 其他种类的图

<img src="image\1676369871870.png" alt="1676369871870" style="zoom:67%;" />

- Unweighted Graph：我们上面讲的都是Unweighted Graph。
- Weighted Graph：他的邻接矩阵并不是非1即0，仍然是对称矩阵，主对角线为0。他的连接数就是所有元素求和除以2。平均连接数计算方法和Unweighted Graph一样。应用场景：人和人之间的关系，普通朋友、好友、密友、敌对等。

<img src="image\1676370202012.png" alt="1676370202012" style="zoom:67%;" />

- Self-edges：带自环的图，仍然是对称矩阵$A_{ij}=A_{ji}$，但是主对角线元素可以不等于0（$A_{ij}\not=0$）.
- 连接总数：首先将主对角线的元素求和，然后再对非主对角线元素求和除以2.

---

- Multi graph：任然是主对角线为0，对称矩阵。
- 连接总数：所有非0元素求和除以2.

## 1.3 图的连通性

**Connected Graph（连通图）**：任意两个顶点可以构成一条路径。

![1676375780221](image\1676375780221.png)

Disconnected Graph（非连通图）由至少两个connected conponents（联通分支）构成。

- Isolated node：孤立节点 =》H
- Giant Component：最大连通分支

<img src="image\1676376208664.png" alt="1676376208664" style="zoom:67%;" />

> Disconnected Graph：由数个分支构成的图的邻接矩阵可以写成分块对角的形式，因此，非零的元素被限制在了方块中，其他元素都是零。
>
> 如果有一个桥接节点将两个连通分支相连，那么就打乱了上面的规律。

<img src="image\1676377080859.png" alt="1676377080859" style="zoom:80%;" />

强连通分量可以被识别，但并不是每一个节点都是非平凡强连通分量的一部分。

- In-component：指向SCC的节点。
- Out-component：由SCC指向的节点。

# 2. 特征工程

​	人工设计特征，将节点、连接、全图作为向量输入到机器学习算法中预测，在后面的章节，这个d维向量是由图神经网络自动学习出来的。

 ![1676532933297](image\1676532933297.png)

​	节点、连接、全图的预测都需要使用机器学习，但是ML的输入不可能是整张图， 计算机看不懂这些花里胡哨的图，他们只能看懂向量、矩阵。所以我们要把节点的特征写成向量，连接的特征写成向量，子图或全图的特征写成向量，再把向量放入ML模型中。不同的节点，不同的连接，堆叠起来之后就变成了矩阵。

​	特征也分为属性特征和连接特征：

- 属性特征

![1676533260329](image\1676533260329.png)

​	一个节点自己就有的特征称之为属性特征，且这个特征是多模态的，比如：社交网络中，一个用户的头像，基本信息，朋友圈的语音，朋友圈的视频。

- 连接特征

![1676533429747](image\1676533429747.png)

​	一个节点在图中和其他节点的连接关系。

![1676533731891](image\1676533731891.png)

> ​	在鸢尾花模型中，这里的D维向量的D=4，花瓣长度、花瓣宽度、花萼长度、花萼宽度；这些特征的质量要足够好，足以让机器学习去区分三种鸢尾花的类别。我们能不能构造一些新的特征，使得我们的算法更容易去区分他们。这就是特征工程。

![1676533968724](image\1676533968724.png)

> 对于抑或型的数据，我们不能用“一根筷子”去区分他们，但是我们添加了新的特征$x_1x_2$之后，就可以简单明了的判断他们的种类。

![1676534097896](image\1676534097896.png)

> 只要特征足够好，最后的结果就不会差。

![1676534253295](image\1676534253295.png)

- 节点层面
- 连接层面
- 子图层面
- 全图层面

## 2.1 节点层面

![1676534343089](image\1676534343089.png)

> 输入某个节点的D维特征，输出他是某类别的概率。
>
> Key：高效的构造D维向量。

![1676534483418](image\1676534483418.png)

> 对节点构造D维特征（连接特征）：
>
> - 节点的度
> - 节点重要性
> - 聚集系数
> - 子图

### 2.1.1 Node Degree

![1676534654236](image\1676534654236.png)

> 节点A和节点G的度数相等，但是他们圈子的质量完全不同。所以我们不能只看节点的度，还要看节点的重要程度。

![1676534792482](image\1676534792482.png)

> 邻接矩阵乘以一个全为1的列向量，得到新的向量就是每一个节点的连接数。我们可以把这个向量作为某一个节点D维向量的某一维特征。如：3号节点的D维特征的第一维可能就是Degree Centrality向量中IDs为3的元素值：3。

Node Degree只看数量不看质量，如何将质量也考虑进去？

### 2.1.2 Node Centrality

![1676535148129](image\1676535148129.png)

> Node Degree 只考虑连接的数量而不考虑质量；
>
> Node Centrality将节点的重要性纳入考虑；
>
> 衡量重要性的方法：
>
> - Eigenvector centrality
> - Betweenness centrality
> - Closeness centrality

#### 2.1.2.1 Eigenvector centrality

![1676535455241](image\1676535455241.png)

> 如果一个节点，其周围的节点很重要，那么这个节点也很重要。
>
> 一个节点的重要度等于与他相邻的节点的重要度求和再归一化。
>
> 这是一个递归的问题，我要求V节点的centrality，我们就要求与之相连的U节点的centrality，要求U节点的centrality就要求与之相连的节点的centrality。

将求eigenvector centrality的问题转换为求邻接矩阵A的特征值，特征向量的问题。

![1676535708474](image\1676535708474.png)

> 上面个的形式，可以等价为求A矩阵的特征向量。
>
> $\lambda$乘以一个C向量，比如说现在途中有4个节点，那么这个C向量就是一个4行1列的向量，代表这4个节点每一个节点的eigenvector centrality；则$\lambda \vec{C}$就等价于临界矩阵乘以$\vec{C}$；
>
> 此时，这个$\vec{C}$向量就是邻接矩阵A的特征向量。
>
> 必定会存在一个为正并且唯一的最大的特征值。

![1676536946678](image\1676536946678.png)

> Node Degree 和Eigenvector Centrality计算出来的结果是明显不一样的，Eigenvector Centrality更偏向于真正的重要节点，哪怕你连接数很多，你也不一定重要。

![1676537124461](image\1676537124461.png)

对上海地铁站进行数据挖掘，可以直白的显示出哪些站很重要。

#### 2.1.2.2 Betweenness Centrality

![1676537297871](image\1676537297871.png)

> 衡量一个节点是在交通咽喉和必经之地。
>
> 如果计算C节点的Betweenness Centrality，就要忽略C节点，查看剩下节点的两两节点对（剩下4个节点，$C_4^2=6$，AB AD AE BD BE DE），计算他们的最短距离，看有多少最短距离一定要经过C节点（AD、AB、AE的最短距离都经过了C）， 此时，C节点的Betweenness Centrality，$C_c=3$.
>
> 看A节点，忽略A节点，同样是6对节点对。这6对里面，没有一对的最短距离经过A，那么A的Betweenness Centrality，$C_A=0$.
>
> 综上，同理：$C_A=C_B=C_E=0$，$C_C=C_D=3$。

![1676538715080](image\1676538715080.png)

> 湖北省到中国任意一个行政省，最多跨越两个省份。
>
> 素有：九省通衢的美称。也就是说湖北的Betweenness Centrality很高。

![1676538929401](image\1676538929401.png)

> 处在很多线路的交汇处，换成肯定要经过这些站，所以Betweenness Centrality也是很高的。

#### 2.1.2.3 Closeness Centrality

![1676539011992](image\1676539011992.png)

> 表示一个节点去哪都近。
>
> 分子：1；
>
> 分母：节点V到其他节点U最短路径长度求和。
>
> 计算A节点的Closeness Centrality：
>
> $C_A=\frac{1}{（A到B的最短路径长度+A到C的最短路径长度+A到D的最短路径长度+A到E的最短路径长度）}$=$\frac{1}{2+1+2+3}=\frac{1}{8}$
>
> 计算D节点的Closeness Centrality：
>
> $C_D=\frac{1}{（D到A的最短路径长度+D到B的最短路径长度+D到C的最短路径长度+D到E的最短路径长度）}$=$\frac{1}{2+1+1+1}=\frac{1}{5}$
>
> $C_D>C_A$，证明节点D节点处在图的中心区域，他去哪里都近；而A节点去哪都远。

![1676539365652](image\1676539365652.png)

> 去哪都近的站还是比较重要的站，他在上海的内环，即中心区域，不管向哪里走都是比较近的。

### 2.1.3 Clustering Coefficient

![1676539871418](image\1676539871418.png)

> 集群系数，衡量一个节点周围有多抱团。
>
> 分母：V节点相邻节点量量对数；
>
> 分子：V节点相邻节点两两也相邻的个数。（以V节点与其相邻两节点构成的三角形的个数）
>
> 如图一：1v3 1v2 2v4 3v4 1v4 2v3 一共6个三角形。

![1676540260791](image\1676540260791.png)

### 2.1.4 Graphlets

![1676540361046](image\1676540361046.png)

> 这种网络成为Ego-Network，有一个中心节点，所有的节点都和这个中心节点相连，外围节点之间可能也相连，这就称之为自我中心网络。
>
> 数三角形个数，也就是计算Clustering Coefficient。
>
> 三角形相当于是我们预先定义的一个子图，我们也可以把三角形换成其他形式的子图，那就称之为Graphlets。

![1676540644430](image\1676540644430.png)

> 2个节点存在1种节点角色；
>
> 3个节点存在2种子图， 这2种子图存在3种节点角色；
>
> 4个节点存在6种子图；
>
> 算出2到5节点所有的节点角色的话，从0到72，有73种节点角色。

![1676544496564](image\1676544496564.png)

> Graphlets：类似于同分异构体，称之为非同形的子图。
>
> 5节点共有73种非同形的节点角色。

![1676544614274](image\1676544614274.png)

> 提取某一个节点周围的Graphlet的子图个数，就可以构建出一个向量，称之为Graphlet Degree Vector（GDV）。
>
> 然后用Graphlets中的a,b,c,d节点代替G中的u节点，观察其出现的次数，由此构建出GDV。
>
> GDV描述了节点u的局部领域拓扑结构信息，每一个节点的GDV肯定是不一样的。
>
> - c维度为什么是0？
>
> 如果把c放置在图G中u的位置，在G中u的两个邻居节点是有连接的，但是在graphlets中，c的邻居节点没有连接，所以不成立。

![1676547244022](image\1676547244022.png)

>类比：
>
>- Degree：数节点的连接数；
>- Clustering coefficient：数节点周围的三角形个数；
>- GDV：数节点周围的Graphlets，即预先定义好的子图模型。

![1676547425972](image\1676547425972.png)

> - 5个节点总共会有73种节点角色，此时的GDV就会有73维向量，它可以充分描述一个节点的拓扑连接结构信息。
> - 比较两个节点的GDV向量就可以计算距离和余弦相似度。

![1676547624835](image\1676547624835.png)

> 在NetworkX里面Graphlets成为Atlas；
>
> 6个节点的所有子图模式都可以画出来，一共有142种。

### 2.1.5 Summary

![1676547780487](image\1676547780487.png)

> - 重要度：
>   - Node Degree
>   - Different node centrality measures
> - 节点的局部邻域拓扑连接结构：
>   - Node Degree
>   - Clustering coefficient
>   - Graphlets count vector

![1676548068240](image\1676548068240.png)

> - 基于重要的的特征：捕获图中每个节点的重要度。
>   - Node Degree：
>     - 简单的计算邻居节点的个数；
>   - Node Centrality：
>     - 衡量邻居节点的重要性；
>     - 不同的衡量策略：
>       - Eigenvector centrality
>       - Betweenness centrality
>       - closeness centrality
> - 分析图中有影响力的节点
>   - 如：预测社交网络中的大V

![1676548375314](image\1676548375314.png)

> - 基于结构的特征：捕获节点周围局部邻域的拓扑属性。
>   - Node Degree：
>     - 数邻居节点个数。
>   - Clustering Coefficient：
>     - 衡量邻居节点有多抱团。
>   - Graphlet degree vector：
>     - 统计不同graphlets出现的次数。
> - 用在预测一个节点在图中扮演的特殊角色。
>   - 分析蛋白质交互的网络中，蛋白质的功能，是否为一个特别重要的枢纽蛋白。

## 2.2 连接层面

![1676549607918](image\1676549607918.png)

> - 通过已知连接捕捉未知连接。
>
> - 在测试时，节点对（没有存在连接）被排序，返回K个可能存在连接的节点对。
>
> - 关键是设计节点对之间的特征。
>
>   - 思路一：直接提取link特征，将link变为D维向量。
>   - 思路二：把link两端节点的D维向量拼接在一起，变成2D维的向量或者变为（2，D）的矩阵。 （会丢失link本身的连接结构信息。）比如：将京沪高速表示成北京的节点的D维向量和上海节点的D维向量简单的拼接，表征京沪高速这个连接，就会丢失北京到上海之间的信息。
>
>   将link变为向量之后输入到下游的机器学习任务中进行预测，是不是可能存在、link本身的类别示什么、他们俩的关系是普通好友、特别关注还是拉黑。所以，既要补全连接，还要猜出连接。

![1676550263465](image\1676550263465.png)

> 连接预测的两种形式：
>
> - 客观的静态图（随时间不变的），比如说蛋白质分子。我们随机的删除掉一些连接，然后尝试把删掉的再预测出来。（半监督问题）
> - 随时间变化的，比如说论文的引用、社交网络、微信好友和学术合作，他是随时间演变的。我们需要用上一个时间区段的图，去预测下一个时间区段的L个连接，从这L个连接中选择出Top N个连接。比较这Top N个连接和下一时间区段的真实连接，来反映出我们预测的结果好还是不好。

![1676550666412](image\1676550666412.png)

> 提取连接特征  ==》 映射成D维向量  ==》  放入到机器学习模型中评估
>
> - 对每个节点对(x,y)计算出分数c(s,y)
>   - 例如：最后得分c(x,y)可以是(x,y)之间的共同邻居的个数。
> - 将(x,y)节点对按照最终的得分c(x,y)从高到低进行排序。
> - 预测出分数最高的Top N个节点对形成新的连接。
> - 观察Graph在下一时间段中真是出现的连接。

![1676551436198](image\1676551436198.png)

- 基于两节点距离
- 基于两节点局部连接信息
- 基于两节点在全图的连接信息

### 2.2.1 Distance-Based Features

![1676551829203](image\1676551829203.png)

> 两节点之间的最短路径长度
>
> - Example：
>
>   B->H = 2 ; B->E = 2 ; A->B = 2 ;
>
>   B->G = 3 ; B->F = 3 ;
>
> - 和 Node Degree一样，只看重长度而忽略了质量。
>
>   比如说 B->H 是有两条路的，院士B既可以通过院士C认识H，也可以通过院士D认识H。
>
>   A和B只能通过C认识。虽然他们的最短路径都是2，但是个数是不一样的，通路的结构是不一样的，所以不能只看最短路径的长度。

![1676552275245](image\1676552275245.png)

> 在之后要说的地铁案例中，可以计算两个节点之间的最短路径长度，比如说“昌吉东路站”到“同济大学”站，中间要换乘的站，以及最短路径的长度。
>
> 还有整个上海地铁网平均最短路径的长度。
>
> 还可以计算某一站去任意其他站的最短路径长度。、
>
> 还可以自己完成一个地铁导航换乘系统。

### 2.2.2 Local Neighborhood Overlap

![1676552741681](image\1676552741681.png)

> 基于两节点的局部连接信息；
>
> 捕获两节点$V_1$、$V_2$之间共同的邻居节点个数。
>
> - 共同邻居：A和B的共同好友个数，只有C一个。
>
>   $|N(A) \cap N(B)|=|\{C\}|=1$.
>
> - 杰卡德系数：计算A和B之间的交并比
>
>   $\frac{|N(A) \cap N(B)|}{|N(A) \cup N(B)|}=\frac{|\{C\}|}{|\{C, D\}|}=\frac{1}{2}$.
>
> - AA指标：A和B的共同好友是不是社牛
>
>   $\sum_{u \in N\left(v_{1}\right) \cap N\left(v_{2}\right)} \frac{1}{\log \left(k_{u}\right)}$
>
>   如果A和B的共同好友是社牛、明星等公众人物，那么他们之间的连接实际上是别较弱的。
>
>   如果A和B的好友是一个深居简出的人，那么他们的友谊就显得情比金坚了。

![1676553567268](image\1676553567268.png)

> 我们在NetworkX里面可以轻松的计算出任意两个节点之间的共同连接的节点、交并比、AA指标。

### 2.2.3 Global Neighborhood Overlap

![1676553710515](image\1676553710515.png)

> Local Neighborhood Overlap的缺点：
>
> 如果两个节点之间没有共同好友，比如说A和E，他们的交并比是0，共同好友个数也是0。所以我们不能看局部近邻领域，而是要看全图的信息。

![1676553849847](image\1676553849847.png)

> 全图的信息我们可以用Katz index来表示。
>
> Katz index：计算节点u和节点v之间长度为K的路径个数。
>
> Q：如何计算Kazt index？
>
> A：使用图的邻接矩阵的幂。

![1676554131041](image\1676554131041.png)

> $P_{uv}^{(K)}$：节点u和节点v之间，长度为K的路径个数。
>
> 比如：$P_{12}^{(1)}=A_{12}=1$.节点1和节点2之间长度为1的路径个数是邻接矩阵中$A_{12}$的值1.

![1676554452925](image\1676554452925.png)

> 当K=2时；
>
> K=2就是将邻接矩阵做平方：$A_{uv}^2$ 之后得到一个新的矩阵。
>
> 我们再观察新矩阵的第一含第二列，即:$A_{12}^2=1$
>
> 这说明节点1和节点2之间，长度为2的路径个数为1。
>
> 1->4->2 : 长度为2.
>
> - 为什么邻接矩阵的平方就得到了长度为2的卡兹系数呢？
>
> $A_{ui}$是与u节点相隔1步的节点i；
>
> $P_{iv}^{(1)}$表示节点i和v是否相隔1步；
>
> 两者相乘就表示u和v相隔2步；

![1676554956403](image\1676554956403.png)

> 通过数学归纳法可以证明：$A_{uv}^{(l)}$就是长度为$l$的卡兹系数。
>
> 节点$u$和节点$v$之间长度为$K$的路径个数就是$A_{uv}^K$矩阵的第$u$行第$v$列的元素。

![1676555204400](image\1676555204400.png)

> 将长度$l$增加到无穷大进行求和：
>
> $A_{v_1v_2}^l$是$v_1$和$v_2$之间，长度为$l$的路径个数。
>
> $\beta^l$（$0<\beta<1$）是折减系数，长度越大，次方的个数越大，折减的也越厉害
>
> 使用高等数学的几何级数，可以将其转换成$\boldsymbol{S}=\sum_{i=1}^{\infty} \beta^{i} \boldsymbol{A}^{i}=(\boldsymbol{I}-\beta \boldsymbol{A})^{-1}-\boldsymbol{I}$的形式。

![1676556340205](image\1676556340205.png)

> 1. 首先根据醉倒特征值确定折减系数；
> 2. 创建单位矩阵
> 3. 计算Katz Index
>
> 数据来源是空手道俱乐部的34名成员，最后得到的Katz Index矩阵是两两队员之间的卡兹系数。

### 2.2.4 Summary

![1676556587366](image\1676556587366.png)

## 2.3 全图层面

![1676556928953](image\1676556928953.png)

> 全图层面的特征工程对应的是全图的图数据挖掘。
>
> 也就是说我们要把整张图变为D维的向量，而且这个D维向量应该能反应全图的结构特点。
>
> 比如上图，存在两个community，D和E之间存在一个桥接枢纽，那么我们的D维向量应该能反应全图的结构特点。

![1676557080576](image\1676557080576.png)

> 对于图而言，我们仍然是在数不同特征在图中出现的个数，这实际上是NLP中的BoW的思想。
>
> 比如一篇文章，我们可以把词典里出现的词都列出来，然后数这篇文章中出现的词频信息。Word1出现了0次，Word2出现了1次，Word3出现了3次...，这样我们就可以把一篇文章变成一个N维向量，N维向量的每一个元素就是对应词出现的次数。文章是由人类的语言文字构成，没法直接输入到ML算法中，但是我们将其转换成向量之后，向量可以输入到ML模型中。
>
> 如果我们把图Graph看作成一篇文章，节点看作是Word。对于一个4个节点构成的图，我们可以转换成4维的向量，【1，1，1，1】第一个节点是否出现，第二个节点是否出现等；这样我们可以发现上面的两张图向量表示是一样的，虽然他们长得不一样，但是他们编码出来的Bag of Nodes是一样的。所以这个Bag of Nodes并不科学，他只看第$i$个节点有没有，并不看他们之间是否有连接。

![1676557510668](image\1676557510668.png)

> 我们将其升级维Bag of Node Degrees。只看Node Degree的个数，不看节点，不看连接结构。
>
> ![1676557589633](image\1676557589633.png)
>
> 用不同颜色，表示不同的Node Degree。
>
> 我们将Bag of * 推广成任意一种特征。

### 2.3.1 Graphlet Features

![1676557658710](image\1676557658710.png)

> 比如说推广到 Graphlets。
>
> 对于3个节点的图，我们有4种Graphlets，从全图的视角分析，就会有$g_4$这种孤立的Graphlets。
>
> 4个节点的图有11种Graphlets。这11种Graphlets都是非同形的。

![1676557838662](image\1676557838662.png)

> 那么我们就可以把全图的不同的Graphlets的个数统计出来作为向量，他和节点的Graphlets存在的区别就是：
>
> 1. 可以存在孤立节点。
> 2. 我们是从全图的视角去看，去数全图的Graphlets，而不是像节点的Graphlets一样去数邻域的Graphlets的个数。

![1676557993862](image\1676557993862.png)

> 假如说存在$n_k$个Graphlets，我们可以构造一个$n_k$维的向量，第$i$ 个元素就是第$i$个Graphlets在全图中出现的次数，

![1676558082648](image\1676558082648.png)

> 对这个图G，我们看节点数为3的Graphlets个数，一共有4种。
>
> 那么对于图G，我们就可以用$f_G=(1,3,6,0)^T$表示。

### 2.3.2 Graphlet Kernel

![1676558307439](image\1676558307439.png)

> 如果我们把两个图的Graphlet Count Vector做数量积，就可以算出这两张图的kernel：
>
> $K(G,G\prime)={f_G}^Tf_{G\prime}$
>
> 这个标量就可以反映出两张图的关系，这两张图是否足够相近，他们两个是否匹配。
>
> 如果两张图不一样大，那么就需要对其做一个归一化，使得他们的大小在一个数量级，这样做出的图才不会有歪曲。
>
> 所以要对每个向量做一个归一化，之后对归一化后的向量做点乘算出Kernel的值。

![1676558598341](image\1676558598341.png)

> 但是这种子图匹配很吃算力。
>
> - 因为我们要在全图去枚举，节点数为k的子图做匹配，他的复杂度是多项式复杂度$n^k$。
> - 这是一个NP-hard的问题。
> - 即使我们将节点的度限制为d，其复杂度也只是稍微减少，为$O(nd^{k-1})$.

#### 2.3.2.1 Weisfeiler-Lehman Kernel

![1676558826894](image\1676558826894.png)

> 使用颜色微调的方法，迭代的去丰富节点的词库。

![1676559129378](image\1676559129378.png)

> 1. 首先将所有节点命名为1；
> 2. 根据节点的连接数，调整命名：
>    - 1个连接 -> 1
>    - 2个连接 -> 11
>    - 3个连接 -> 111
>    - 4个连接 -> 1111

![1676559283439](image\1676559283439.png)

> 1. 将1，11，111，1111...用不同的数字表示。
> 2. ![1676559344681](image\1676559344681.png)
> 3. 根据Hash Table转换颜色。

![1676559389144](image\1676559389144.png)

> 再对图进行类似的操作：
>
> 1. 根据连接关系，更改命名

![1676559512526](image\1676559512526.png)

> 再次进行Hash映射：
>
> 1. 统计上面出现的颜色：4,345、3,44、5,2244等，设计Hash Table
> 2. 根据Hash Table进行转换。
>
> 由图可以看出：
>
> ![1676559732268](image\1676559732268.png)
>
> 左图编码为7的两个叶子节点颜色是永远相同的，因为他们和12号节点单线连接。

![1676559749998](image\1676559749998.png)

> ![1676559875835](image\1676559875835.png)
>
> 所以：1对应元素6
>
> ![1676559916783](image\1676559916783.png)
>
> 所以：2对应2；3对应1；4对应2；5对应1
>
> ![1676559965465](image\1676559965465.png)
>
> 所以：6对应0；7对应2；8对应1；9、10对应0；11对应2；12对应1
>
> 综上：我们就把图一和图二编码成了13维向量。
>
> 有些元素为0，就说明再这张图中没有出现，在另一张图中出现。

![1676560538798](image\1676560538798.png)

> 使用点乘计算Kernel。

![1676560591909](image\1676560591909.png)

> 1. 先进行：Hash（每一个节点的颜色，周围节点的颜色）
> 2. 将Hash之后的值（如：4，345）补充到词汇表中，并对其进行编号
> 3. 然后数全图中有多少个这样的编号。
>
> 进行K次颜色微调就捕捉到了K-hop（K跳）的连接，K越大，捕捉的范围就越广。

![1676560890641](image\1676560890641.png)

> WL Kernel算法在计算上是十分高效的。计算复杂度与连接个数呈线性相关。
>
> 在做内积的时候，我们可以忽略为0的元素，只对两幅图G中都出现的颜色进行内积。

### 2.3.3 Summary

![1676561117973](image\1676561117973.png)

> - Graphlet Kernel
>   - Graph被表征为Bag of Graphlets
>   - 计算代价很大
> - WL Kernel
>   - Graph被表征为Bag of Colors
>   - 计算高效
>   - 与GNN关系密切

![1676561235015](image\1676561235015.png)

> 把两张图的向量做内积为一个标量，这种方法称之为核方法。
>
> Kernel Methods被广泛应用全图层面的图机器学习。因为经常是两个图层面做比较：两张大图、一张大图和一张子图，都可以用Kernel Methods。此时就是用Kernel这个标量来表示两张图的结构了，而不是用一张图的向量来代表一张图的结构了。
>
> - 两张图的Kernel是一个标量。
> - 有了kernel之后，一些已有的图机器学习方法，比如说SVM就可以用了。