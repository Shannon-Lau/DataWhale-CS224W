# 1. 节点嵌入

如何把节点映射成D维向量？

- 人工特征工程：节点重要度、集群系数、Graphlet

- 图表示学习：通过随机游走构造子监督学习任务。DeepWalk、Node2Vec

  不需要人工提取特征，不需要特征工程，通过随机游走的方法在图中采样，再通过随机游走自动的学习每个节点的嵌入和向量。代表算法就是DeepWalk、Node2Vec和Line。

- 矩阵分解：图表示学习也可以统一成矩阵分解的形式。
- 深度学习：图神经网络

![image-20230220210738103](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202107138.png)

1. 概述

将节点映射成D维的向量。

![image-20230220203219993](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202107849.png)

传统的机器学习算法需要我们自己设计特征，通过特征工程设计出一个足够优秀的特征之后再喂给机器学习模型，本质上和鸢尾花、波士顿房价等任务没有区别。

![image-20230220203521225](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202107689.png)

图表示学习自动学习特征，把各个模态（视频、文本、语言、图像等）的输入转为向量。这个向量保留原有信息，并且携带了便于下游机器学习任务的信息，这个过程是全自动，端到端，不需要人工干预的。它避免了做特征工程的工作。

![image-20230220210700866](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202107912.png)

D维向量：

- 低维：向量维度远小于节点数。
- 连续：每个元素都是实数。
- 稠密：每个元素都不为0.

这个向量和下游任务无关，不论是要解决节点层面、连接层面、图层面还是子图层面，这个向量是上游直接得到的。

这个过程是无监督的，我们没有用到任何标签，我们通过无监督的方法，将其表示成D维向量。

## 1.1 Why Embedding

![image-20230220210607295](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202106339.png)

这个D维向量相当于D维空间中的一个点。

- 向量的相似度反映了他们在原图中，节点的相似度。两节点如果是直接相邻的话，那么他们就很应该是很相似的。
- 嵌入向量包含了网络的连接信息，而且能够用于下游的各种任务，虽然这个向量的产生本身是与下游任务无关的，但是他这个向量确实反映了这个节点在原图中的信息，可以被用于各种下游任务，不管是节点分类也好，连接预测也好，整个图的分类也好，异常检测也好，还是社群聚类也好，都可以用这个特征，所以这是一个万金油。

![image-20230220210643044](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202106090.png)

> 这是一个著名的空手道俱乐部的数据集，每一个节点，表示了这个空手道俱乐部的一个会员。我们把每一个节点都映射成了二维空间中的一个点，然后我们在原图中给每个节点一个颜色，当然这个向量本身是与颜色无关的。当然我们刚说了是自监督的方式，那我们会发现通过DeepWalk嵌入之后，他们在二维空间中不同颜色的点也是被区分开的。那如果我们要构建一个分类模型，比如说红色和青蓝色是一个门派，紫色和绿色是一个门派，那其实我们只需要拟合一个线性可分的决策边界就可以了。
> 所以说图嵌入使得下游的机学习任务变简单了。输入是这么复杂的一个图，而现在只需要对这些结构化的向量来进行机器学习、拟合决策边界就可以了。原图中相近的节点嵌入后依然相近。这就是DeepWalk的魔力，那当然DeepWalk本身是与这些颜色无关的，那这些向量的结构也是与标签无关的，是完全自监督或者叫无监督的。

## 1.2 Encoder-Decoder

​		图嵌入它是有一个基本框架的，就是编码器-解码器框架。它可以适用于CS224W所有的主线任务，都可以用编码器-解码器这个结构来解释。

![image-20230220211550088](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202115130.png)

> - V：节点集合
> - A：邻接矩阵
> - 无权图，A中的元素非0即1
>
> 我们仍然只利用节点的连接信息，没有使用属性信息。只考虑社交的连接关系，不考虑本身的特征（学历、身高、颜值等）。
>
> “人的本质是一切社会关系的总和” 所以看连接信息其实就已经能看一个节点的本质了

![image-20230220211953622](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202119666.png)

>编码器：
>就是编码器这个Encoder就是输入一个节点->输出一个向量
>那输出这个u节点对应的D维向量就叫做Encoder编码器。
>解码器：
>解码器就是把这两个向量做点乘，其实就是它的数量积，余弦相似度得到的值是一个标量。这个标量的值应该能够反映节点的相似度，而这个节点相似是需要人为定义的。不妨先理解成：如果两个节点直接相连，他们就是相似的。那如果两个向量完全不相似呢？那其实就是这两个向量是正交的、是垂直的。那它的向量点成数值数量级应该是0。而如果两个向量是直接相连的，比如说这里的u和v，那他们的就是相似度就是1，代表非常相似，那他的点乘的数值应该就是1，对应的就是这两个向量是平行的，或者说他们甚至就是同一个向量或是共线的。
>
> 
>
>编码器和解码器的结构可以换。比如说我们后边会用图神经网络，用深度学习来做编码器，也是输入一个节点生成他的D维向量。只不过现在是用更深的编码器来实现了，那解码器也可以自己定义。我们这里用的是点乘，也可以计算两向量的L2距离；各种各样的距离度量指标都可以用来计算；也可以换各种各样的相似度的定义。也可以说两个向量直接相连是相似的；也可以说他们有共同好友是相似的；也可以说他们在途中扮演了相同的结构、功能或者角色是相似的。比如说马六甲海峡和巴拿马运河他们两个虽然在地球上隔了几千公里，但是他们的角色是相似的，都是交通的咽喉要道。所以你可以自己定义向量的距离度量指标。

## 1.3 如何定义Node Similarity

![image-20230220213651056](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202136102.png)

> 那么如何去定义节点的相似度：
> 你可以直接定义两个节点直接相连视为相似；
> 两个节点间接相连，就是有一个共同好友，那么可以视为相似；
> 或者有相同的功能角色...
>
> 那在本讲，我们重点讲的是基于随机游走的方法，就是在图里边随机去进行游走，像一个醉汉一样随机去游走，采样得到一个节点序列。如果两个节点共同出现在了这个序列中，那就认为这两向量是相似的

## 1.4 Encoder-Decoder

![image-20230220214049622](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202140668.png)

> 1. 编码器：节点变成D维的向量
> 2. 解码器就是两个向量的数量积能够反映他们的相似度。
> 3. 我们的目标就是迭代的优化每个节点的D维向量：使得图中相似节点的的向量数量积大；不相似节点的向量数量积小。我们要紧紧揪住这个牛尾巴，紧紧咬住这个目标不放，才能够迭代的去优化每个节点的向量，使得最后达到这个结果。
> 4. 解码器输入的是数量积的结果，输出的是相似度。

![image-20230220214557663](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202145708.png)

> 编码器输入节点得到一个D维向量。
>
> 然后我们需要自己定义一个节点相似的指标，然后用数量积去反映这个相似度的指标。那这个结构称之为解码器Decoder。

## 1.5 "Shallow" Encoding

![image-20230220215015114](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202150161.png)

> 最简单的一种编码器其实就是查表，我们强行的把所有节点的D维向量先写在一个矩阵（表）里。
> 比如说我们写成一个z矩阵，z矩阵是d行节点个数列的。现在就是5行6列的一个矩阵，就表示我们把每个节点映射成一个5维的向量，现在有6个节点。
>
> 那如果我想取出某个节点的向量该怎么办呢？
> 就乘以一个v向量，这个v向量是一个one hot encoding，只有一个元素是1，其他都是0。
>
> 比如说我们现在想取出它的第4列，那么这个v向量的第四个元素就是1。这个矩阵乘以这个向量，其实就是把矩阵的第4列取出来了，也就是把第四个节点的嵌入向量取出来了。
>
> 最简单的一种编码器就是查表，查表是最方便的。不需要什么神经网络、不需要什么模型。直接去查表就可以了。那我们的目标就是迭代的优化这5行6列的这30个元素，使得完成我们的优化目标。
>
> 现在可学习的参数就是指z矩阵的这30个参数，那这种查表的方法也被称作浅编码器，它是相对于神经网络的深编码器对应。
>
> 深编码器是需要一个模型，把节点输入到这个模型中，然后这个模型输出一个向量。而现在我们是直接把所有节点的向量直接写出来了。所以现在这种查表的方法是浅编码器，而使用深度学习方法是深编码器。

![image-20230220215938737](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202159788.png)

> 这个z矩阵每一列就表示一个节点的向量，有多少个节点就有多少个列；行数是向量的维度

![image-20230220221324661](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202213707.png)

> 那我们可以直接优化z矩阵，算法可以使用DeepWalk和Node2Vec这两个算法。

![image-20230220221421092](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202214135.png)

> 浅编码器就是查表的方法，之后会说深编码器，就是用图神经网络来进行编码。
> z矩阵就包含了所有节点的向量。那我们的目标就是迭代优化，z矩阵中每一个元素，使得图中相似节点的向量数量极大；不相似节点的向量数量极小。

![image-20230220221630425](https://cdn.jsdelivr.net/gh/Shannon-Lau/DataWhale-CS224W@main/image/202302202216470.png)

> 这是一个无监督和自监督的问题。
>
> ​	因为我们没有使用类别标签。
>
> ​	也没有使用节点的属性特征
>
> ​	直接优化这个嵌入项量。
>
> 我们通过随机游走的方法定义了节点的相似指标，如果两个节点出现在了同一个随机游走序列中就说明这两节点是相似的，而且它是与下游任务无关的，因为我们整个优化过程，都没有考虑到要解决任何下游任务。虽然没有针对特定的下游任务，但是他是万金油，等他学习好了可以被用于下游任务，但是并没有针对某种特定的下游任务。
